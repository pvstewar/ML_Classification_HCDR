{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4- Pytorch Lightning Implementation \n",
    "\n",
    "Created by Michael Casey, Andrew Cruez, Peter Stewart, and Hemraj Yadav"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__About Part 4__\n",
    "\n",
    "A continuation of the HCDR classification project, this part will focus on implimentation of an artificail neural network using Pytorch. We will use the same datasets created in part 2 but in this case we'll model exclusively using PyTorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning \n",
    "import pytorch_lightning as pl\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from lightning import Callback\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchmetrics import Accuracy\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# from torchvision import transforms\n",
    "sys.path.insert(0, '..')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from part 3\n",
    "DATA_DIR = \"./Data/home-credit-default-risk\"\n",
    "\n",
    "with open(f\"{DATA_DIR}/X_train.pkl\", 'rb') as aa:\n",
    "    X_train = pickle.load(aa)\n",
    "\n",
    "with open(f\"{DATA_DIR}/X_test.pkl\", 'rb') as bb:\n",
    "    X_test = pickle.load(bb)\n",
    "\n",
    "with open(f\"{DATA_DIR}/y_train.pkl\", 'rb') as cc:\n",
    "    y_train = pickle.load(cc)\n",
    "\n",
    "with open(f\"{DATA_DIR}/y_test.pkl\", 'rb') as dd:\n",
    "    y_test = pickle.load(dd)\n",
    "\n",
    "with open(f\"{DATA_DIR}/X_valid.pkl\", 'rb') as ee:\n",
    "    X_valid = pickle.load(ee)\n",
    "\n",
    "with open(f\"{DATA_DIR}/y_valid.pkl\", 'rb') as ff:\n",
    "    y_valid = pickle.load(ff)\n",
    "\n",
    "with open(f\"{DATA_DIR}/X_test_SA.pkl\", 'rb') as gg:\n",
    "    X_test_SA = pickle.load(gg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train col number is: 1297\n",
      "X_test col number is: 1297\n",
      "X_valid col number is: 1297\n",
      "Total of num and cat columns is: 1297\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 209107 entries, 35339 to 212146\n",
      "Columns: 1297 entries, NAME_CONTRACT_TYPE to SUM(previous_FT.SELLERPLACE_AREA)\n",
      "dtypes: float64(1233), int64(44), object(20)\n",
      "memory usage: 2.0+ GB\n"
     ]
    }
   ],
   "source": [
    "#Now let's determine which features are categorical or numerical for our X_train_FT dataset\n",
    "numerical_at = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_at = X_train.select_dtypes(include=['object', 'bool']).columns\n",
    "X_train[categorical_at]=X_train[categorical_at].astype(str)\n",
    "X_test[categorical_at]=X_test[categorical_at].astype(str)\n",
    "X_valid[categorical_at]=X_valid[categorical_at].astype(str)\n",
    "tot_from_above = (len(numerical_at)+len(categorical_at))\n",
    "print (f\"X_train col number is: {X_train.shape[1]}\")\n",
    "print (f\"X_test col number is: {X_test.shape[1]}\")\n",
    "print (f\"X_valid col number is: {X_valid.shape[1]}\")\n",
    "print (f\"Total of num and cat columns is: {tot_from_above}\")\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BL_num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "BL_cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('ohe', OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "# MNB_num_pipeline = Pipeline([\n",
    "#         ('imputer', SimpleImputer(strategy='constant',fill_value= 0)),\n",
    "#         ('scaler', MinMaxScaler()),\n",
    "#     ])\n",
    "\n",
    "# new_num_pipeline = Pipeline([\n",
    "#         ('imputer', SimpleImputer(strategy='constant',fill_value= 0)),\n",
    "#         ('scaler', MinMaxScaler()),\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch_pipeline = ColumnTransformer(transformers=[\n",
    "       (\"num_pipeline\", BL_num_pipeline, numerical_at),\n",
    "       (\"cat_pipeline\", BL_cat_pipeline, categorical_at)],\n",
    "       remainder='drop',\n",
    "        n_jobs=1\n",
    "   )\n",
    "X_train_transformed=  torch_pipeline.fit_transform(X_train)\n",
    "\n",
    "\n",
    "column_names = list(numerical_at) + list(torch_pipeline.transformers_[1][1].named_steps[\"ohe\"].get_feature_names_out(categorical_at))\n",
    "X_train_transformed= (pd.DataFrame(X_train_transformed,  columns=column_names))\n",
    "number_of_inputs = X_train_transformed.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed= torch_pipeline.fit_transform(X_test)\n",
    "\n",
    "\n",
    "column_names = list(numerical_at) + list(torch_pipeline.transformers_[1][1].named_steps[\"ohe\"].get_feature_names_out(categorical_at))\n",
    "X_test_transformed= (pd.DataFrame(X_test_transformed,  columns=column_names))\n",
    "number_of_inputs = X_test_transformed.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_transformed= torch_pipeline.fit_transform(X_valid)\n",
    "\n",
    "\n",
    "column_names = list(numerical_at) + list(torch_pipeline.transformers_[1][1].named_steps[\"ohe\"].get_feature_names_out(categorical_at))\n",
    "X_valid_transformed= (pd.DataFrame(X_valid_transformed,  columns=column_names))\n",
    "number_of_inputs = X_valid_transformed.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(209107, 1431)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_list = (X_train_transformed.columns.difference(X_test_transformed.columns))\n",
    "print(len(drop_list))\n",
    "drop_list\n",
    "X_train_transformed = X_train_transformed.drop(drop_list, axis=1)\n",
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(209107, 1429)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_list = (X_train_transformed.columns.difference(X_valid_transformed.columns))\n",
    "print(len(drop_list))\n",
    "X_train_transformed = X_train_transformed.drop(drop_list, axis=1)\n",
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(46127, 1429)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_list = (X_test_transformed.columns.difference(X_valid_transformed.columns))\n",
    "print(len(drop_list))\n",
    "X_test_transformed = X_test_transformed.drop(drop_list, axis=1)\n",
    "X_test_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209107, 1429)\n",
      "(46127, 1429)\n",
      "(52277, 1429)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_transformed.shape)\n",
    "print(X_test_transformed.shape)\n",
    "print(X_valid_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed.to_csv(f'{DATA_DIR}/X_train_pt.csv',index=False)\n",
    "X_test_transformed.to_csv(f'{DATA_DIR}/X_test_pt.csv',index=False)\n",
    "X_valid_transformed.to_csv(f'{DATA_DIR}/X_valid_pt.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_csv(f'{DATA_DIR}/y_train_pt.csv',index=False)\n",
    "y_test.to_csv(f'{DATA_DIR}/y_test_pt.csv',index=False)\n",
    "y_valid.to_csv(f'{DATA_DIR}/y_valid_pt.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209107, 1429)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(pl.LightningModule):\n",
    "    def __init__(self, input_size=(1429), hidden_units=(32, 16)):\n",
    "        super().__init__()\n",
    "        \n",
    "        # new PL attributes:\n",
    "        self.train_acc = Accuracy(\"binary\", num_classes=2)  #change to binary class and 2 for HCDR\n",
    "        self.valid_acc = Accuracy(\"binary\", num_classes=2)\n",
    "        self.test_acc = Accuracy(\"binary\",num_classes=2)\n",
    "        \n",
    "        # Model similar to previous section:\n",
    "        input_size = input_size\n",
    "        all_layers = [nn.Linear(input_size, hidden_units[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_units[0], hidden_units[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_units[1], 2)\n",
    "                      ]\n",
    "        # for hidden_unit in hidden_units: \n",
    " \n",
    "        # all_layers.append(nn.Linear(hidden_units[-1], 10)) \n",
    "        self.model = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # add in a loss layer\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y.squeeze().long())\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc.update(preds, y.squeeze().long())\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outs):\n",
    "        self.log(\"train_acc\", self.train_acc.compute())\n",
    "        self.train_acc.reset()\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y.squeeze().long())\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.valid_acc.update(preds, y.squeeze().long())\n",
    "        self.log(\"valid_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_epoch_end(self, outs):\n",
    "        self.log(\"valid_acc\", self.valid_acc.compute(), prog_bar=True)\n",
    "        self.valid_acc.reset()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y.squeeze().long())\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.test_acc.update(preds, y.squeeze().long())\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", self.test_acc.compute(), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22c9243b5f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class HCDR_DataModule(pl.LightningDataModule):\n",
    "    \"\"\"Sample Data module for HCDR\n",
    "    Load data from CSV and convert into a numpy array, \n",
    "    then into Pytorch Tensor \n",
    "    and then into a Tensor Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path=f\"{DATA_DIR}/\", batchsize = 2**14):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.batchsize = batchsize\n",
    "        #self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        pass \n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "\n",
    "        df = pd.read_csv(self.data_path + \"X_train_pt.csv\")\n",
    "        df.reset_index()\n",
    "        df.fillna(0)\n",
    "        df = df.astype(np.float32)\n",
    "        X = torch.Tensor(df.values)\n",
    "        df = pd.read_csv(self.data_path + \"y_train_pt.csv\")\n",
    "        df.reset_index()\n",
    "        np.nan_to_num(df, nan=0)\n",
    "        df = df.astype(np.float32)\n",
    "        y = torch.Tensor(df.values.reshape(-1,1))\n",
    "        self.train = torch.utils.data.TensorDataset(X, y)\n",
    "        df = pd.read_csv(self.data_path + \"X_valid_pt.csv\")\n",
    "        df.reset_index()\n",
    "        df.fillna(0)\n",
    "        df = df.astype(np.float32)\n",
    "        X = torch.Tensor(df.values)\n",
    "        df = pd.read_csv(self.data_path + \"y_valid_pt.csv\")\n",
    "        df.reset_index()\n",
    "        np.nan_to_num(df, nan=0)\n",
    "        df = df.astype(np.float32)      \n",
    "        y = torch.Tensor(df.values.reshape(-1,1))\n",
    "        self.val = torch.utils.data.TensorDataset(X, y)\n",
    "        df = pd.read_csv(self.data_path + \"X_test_pt.csv\")\n",
    "        df.reset_index()\n",
    "        df.fillna(0)\n",
    "        df = df.astype(np.float32)      \n",
    "        X = torch.Tensor(df.values)\n",
    "        df = pd.read_csv(self.data_path + \"y_test_pt.csv\")\n",
    "        df.reset_index()\n",
    "        np.nan_to_num(df, nan=0)\n",
    "        df = df.astype(np.float32)\n",
    "        y = torch.Tensor(df.values.reshape(-1,1))\n",
    "        self.test = torch.utils.data.TensorDataset(X, y)\n",
    " \n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.batchsize, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=self.batchsize, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=self.batchsize, num_workers=4)\n",
    "    \n",
    "    \n",
    "torch.manual_seed(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no gpu\n"
     ]
    }
   ],
   "source": [
    "#My machine has an AMD CPU so apparently I need to setup my ML system to run on WSL since AMD rocm isn't windows native.\n",
    "\n",
    "if torch.cuda.is_available(): # if you have GPUs\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"no gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type           | Params\n",
      "---------------------------------------------\n",
      "0 | train_acc | BinaryAccuracy | 0     \n",
      "1 | valid_acc | BinaryAccuracy | 0     \n",
      "2 | test_acc  | BinaryAccuracy | 0     \n",
      "3 | model     | Sequential     | 46.3 K\n",
      "---------------------------------------------\n",
      "46.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "46.3 K    Total params\n",
      "0.185     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095a0d0c2a044afe823cf3d370e3b416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pvste\\anaconda3\\envs\\my_ml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ec3a621c9f49b3ac26d803a457d750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e239f2ee69a4e4f965d3a38ca8c98bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4e9fcd3c1f45b7a1bfec648ee163d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890e796ec0bb4715b1806b3bbff6717d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8b677838fb4b59841e9d3e19735efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1184a5ed7f964512a84eb80ba8307426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5fc7c4c5764672ac33174118dd7478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9a8015976b4531841db3d9f3bff9e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0af174d7a547a18789b0efe5a10a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984bf2ca49b7422ca81de3be5d138672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df75d1fea48e4083925df8bc40a119f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "hcdrclassifier = MultiLayerPerceptron()\n",
    "\n",
    "callbacks = [ModelCheckpoint(save_top_k=1, mode='max', monitor=\"valid_acc\")] # save top 1 model\n",
    "\n",
    "if torch.cuda.is_available(): # if you have GPUs\n",
    "    print(\"GPU is available\")\n",
    "    trainer = pl.Trainer(max_epochs=10, callbacks=callbacks, gpus=1)\n",
    "else:\n",
    "    trainer = pl.Trainer(max_epochs=10, callbacks=callbacks)\n",
    "\n",
    "trainer.fit(model=hcdrclassifier, datamodule=HCDR_DataModule())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at c:\\Users\\pvste\\Documents\\code\\lightning_logs\\version_7\\checkpoints\\epoch=7-step=104.ckpt\n",
      "Loaded model weights from checkpoint at c:\\Users\\pvste\\Documents\\code\\lightning_logs\\version_7\\checkpoints\\epoch=7-step=104.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9496c71bc647bda3595239b1eccc25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9186195731163025     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2512054741382599     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9186195731163025    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2512054741382599    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.2512054741382599, 'test_acc': 0.9186195731163025}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=hcdrclassifier, datamodule=HCDR_DataModule(), ckpt_path='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import AUROC\n",
    "class MultiLayerPerceptron3(pl.LightningModule):\n",
    "    def __init__(self, input_size=(1429), hidden_units=(32, 16)):\n",
    "        super().__init__()\n",
    "        \n",
    "        # new PL attributes:\n",
    "        self.train_acc = Accuracy(\"binary\", num_classes=2)  #change to binary class and 2 for HCDR\n",
    "        self.train_auc = AUROC(\"binary\", num_classes=2)\n",
    "        self.test_acc = Accuracy(\"binary\",num_classes=2)\n",
    "        self.test_auc = AUROC(\"binary\", num_classes=2)\n",
    "        self.valid_AUC = AUROC(\"binary\", num_classes=2)\n",
    "        \n",
    "        # Model similar to previous section:\n",
    "        input_size = input_size\n",
    "        all_layers = [nn.Linear(input_size, hidden_units[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_units[0], hidden_units[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_units[1], 2)\n",
    "                      ]\n",
    "        # for hidden_unit in hidden_units: \n",
    " \n",
    "        # all_layers.append(nn.Linear(hidden_units[-1], 10)) \n",
    "        self.model = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # add in a loss layer\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y.squeeze().long())\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc.update(preds, y.squeeze().long())\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outs):\n",
    "        self.log(\"train_acc\", self.train_acc.compute())\n",
    "        self.log(\"train_auc\", self.train_auc.compute())\n",
    "        self.train_acc.reset()\n",
    "        self.train_auc.reset()\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y.squeeze().long())\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.valid_AUC.update(preds, y.squeeze().long())\n",
    "        self.log(\"valid_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_epoch_end(self, outs):\n",
    "        self.log(\"valid_AUC\", self.valid_AUC.compute(), prog_bar=True)\n",
    "        self.valid_AUC.reset()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y.squeeze().long())\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.test_acc.update(preds, y.squeeze().long())\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", self.test_acc.compute(), prog_bar=True)\n",
    "        self.log(\"test_auc\", self.test_auc.compute(), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type           | Params\n",
      "---------------------------------------------\n",
      "0 | train_acc | BinaryAccuracy | 0     \n",
      "1 | train_auc | BinaryAUROC    | 0     \n",
      "2 | test_acc  | BinaryAccuracy | 0     \n",
      "3 | test_auc  | BinaryAUROC    | 0     \n",
      "4 | valid_AUC | BinaryAUROC    | 0     \n",
      "5 | model     | Sequential     | 46.3 K\n",
      "---------------------------------------------\n",
      "46.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "46.3 K    Total params\n",
      "0.185     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fba46b7643e4626ae153bdd6e5f9e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pvste\\anaconda3\\envs\\my_ml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac77d4c41ded4b63a82092cb89fa43ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4b8ffb1f584081ac4651a1e30f508e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pvste\\anaconda3\\envs\\my_ml\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: The ``compute`` method of metric BinaryAUROC was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No samples to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, callbacks\u001b[39m=\u001b[39mcallbacks)\n\u001b[1;32m---> 11\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model\u001b[39m=\u001b[39;49mhcdrclassifier, datamodule\u001b[39m=\u001b[39;49mHCDR_DataModule())\n",
      "File \u001b[1;32mc:\\Users\\pvste\\anaconda3\\envs\\my_ml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    606\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[0;32m    607\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m--> 608\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    609\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    610\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\pvste\\anaconda3\\envs\\my_ml\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[1;32mc:\\Users\\pvste\\anaconda3\\envs\\my_ml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    643\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    644\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[0;32m    645\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[0;32m    646\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    647\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    648\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    649\u001b[0m )\n\u001b[1;32m--> 650\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[0;32m    652\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pvste\\anaconda3\\envs\\my_ml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[0;32m   1110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m-> 1112\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m   1114\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[1;32mc:\\Users\\pvste\\anaconda3\\envs\\my_ml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1191\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m   1190\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1191\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[1;32mc:\\Users\\pvste\\anaconda3\\envs\\my_ml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1214\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[0;32m   1213\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1214\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32mc:\\Users\\pvste\\anaconda3\\envs\\my_ml\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_advance_end()\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\pvste\\anaconda3\\envs\\my_ml\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:283\u001b[0m, in \u001b[0;36mFitLoop.on_advance_end\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    276\u001b[0m epoch_end_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch_loop\u001b[39m.\u001b[39m_prepare_outputs_training_epoch_end(\n\u001b[0;32m    277\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs,\n\u001b[0;32m    278\u001b[0m     lightning_module\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m    279\u001b[0m     num_optimizers\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizers),\n\u001b[0;32m    280\u001b[0m )\n\u001b[0;32m    281\u001b[0m \u001b[39m# run lightning module hook training_epoch_end\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[39m# refresh the result for custom logging at the epoch level\u001b[39;00m\n\u001b[1;32m--> 283\u001b[0m epoch_end_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\u001b[39m\"\u001b[39;49m\u001b[39mtraining_epoch_end\u001b[39;49m\u001b[39m\"\u001b[39;49m, epoch_end_outputs)\n\u001b[0;32m    284\u001b[0m \u001b[39mif\u001b[39;00m epoch_end_outputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    285\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\n\u001b[0;32m    286\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`training_epoch_end` expects a return of None. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mHINT: remove the return statement in `training_epoch_end`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\pvste\\anaconda3\\envs\\my_ml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1356\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[1;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1353\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[0;32m   1355\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1356\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1358\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1359\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "Cell \u001b[1;32mIn[27], line 42\u001b[0m, in \u001b[0;36mMultiLayerPerceptron3.training_epoch_end\u001b[1;34m(self, outs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_epoch_end\u001b[39m(\u001b[39mself\u001b[39m, outs):\n\u001b[0;32m     41\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mtrain_acc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_acc\u001b[39m.\u001b[39mcompute())\n\u001b[1;32m---> 42\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mtrain_auc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_auc\u001b[39m.\u001b[39;49mcompute())\n\u001b[0;32m     43\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_acc\u001b[39m.\u001b[39mreset()\n\u001b[0;32m     44\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_auc\u001b[39m.\u001b[39mreset()\n",
      "File \u001b[1;32mc:\\Users\\pvste\\anaconda3\\envs\\my_ml\\lib\\site-packages\\torchmetrics\\metric.py:530\u001b[0m, in \u001b[0;36mMetric._wrap_compute.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[39m# compute relies on the sync context manager to gather the states across processes and apply reduction\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[39m# if synchronization happened, the current rank accumulated states will be restored to keep\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[39m# accumulation going if ``should_unsync=True``,\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msync_context(\n\u001b[0;32m    526\u001b[0m     dist_sync_fn\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist_sync_fn,\n\u001b[0;32m    527\u001b[0m     should_sync\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_sync,\n\u001b[0;32m    528\u001b[0m     should_unsync\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_unsync,\n\u001b[0;32m    529\u001b[0m ):\n\u001b[1;32m--> 530\u001b[0m     value \u001b[39m=\u001b[39m compute(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    531\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_computed \u001b[39m=\u001b[39m _squeeze_if_scalar(value)\n\u001b[0;32m    533\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_computed\n",
      "File \u001b[1;32mc:\\Users\\pvste\\anaconda3\\envs\\my_ml\\lib\\site-packages\\torchmetrics\\classification\\auroc.py:109\u001b[0m, in \u001b[0;36mBinaryAUROC.compute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthresholds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 109\u001b[0m         state \u001b[39m=\u001b[39m [dim_zero_cat(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreds), dim_zero_cat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget)]\n\u001b[0;32m    110\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m         state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfmat\n",
      "File \u001b[1;32mc:\\Users\\pvste\\anaconda3\\envs\\my_ml\\lib\\site-packages\\torchmetrics\\utilities\\data.py:29\u001b[0m, in \u001b[0;36mdim_zero_cat\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     27\u001b[0m x \u001b[39m=\u001b[39m [y\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m) \u001b[39mif\u001b[39;00m y\u001b[39m.\u001b[39mnumel() \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m y \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m x]\n\u001b[0;32m     28\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m x:  \u001b[39m# empty list\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo samples to concatenate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(x, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: No samples to concatenate"
     ]
    }
   ],
   "source": [
    "hcdrclassifier = MultiLayerPerceptron3()\n",
    "\n",
    "callbacks = [ModelCheckpoint(save_top_k=1, mode='max', monitor=\"test_auc\")] # save top 1 model\n",
    "\n",
    "if torch.cuda.is_available(): # if you have GPUs\n",
    "    print(\"GPU is available\")\n",
    "    trainer = pl.Trainer(max_epochs=10, callbacks=callbacks, gpus=1)\n",
    "else:\n",
    "    trainer = pl.Trainer(max_epochs=10, callbacks=callbacks)\n",
    "\n",
    "trainer.fit(model=hcdrclassifier, datamodule=HCDR_DataModule())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtest(model\u001b[39m=\u001b[39;49mhcdrclassifier, datamodule\u001b[39m=\u001b[39;49mHCDR_DataModule(), ckpt_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbest\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\pvste\\anaconda3\\envs\\my_ml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:794\u001b[0m, in \u001b[0;36mTrainer.test\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    792\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[0;32m    793\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m--> 794\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    795\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_test_impl, model, dataloaders, ckpt_path, verbose, datamodule\n\u001b[0;32m    796\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\pvste\\anaconda3\\envs\\my_ml\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[1;32mc:\\Users\\pvste\\anaconda3\\envs\\my_ml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:835\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[39m# links data to the trainer\u001b[39;00m\n\u001b[0;32m    833\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(model, test_dataloaders\u001b[39m=\u001b[39mdataloaders, datamodule\u001b[39m=\u001b[39mdatamodule)\n\u001b[1;32m--> 835\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_checkpoint_connector\u001b[39m.\u001b[39;49m_set_ckpt_path(\n\u001b[0;32m    836\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate\u001b[39m.\u001b[39;49mfn, ckpt_path, model_provided\u001b[39m=\u001b[39;49mmodel_provided, model_connected\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m    837\u001b[0m )\n\u001b[0;32m    839\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tested_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mckpt_path  \u001b[39m# TODO: remove in v1.8\u001b[39;00m\n\u001b[0;32m    841\u001b[0m \u001b[39m# run test\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pvste\\anaconda3\\envs\\my_ml\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:150\u001b[0m, in \u001b[0;36mCheckpointConnector._set_ckpt_path\u001b[1;34m(self, state_fn, ckpt_path, model_provided, model_connected)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mfast_dev_run:\n\u001b[0;32m    146\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    147\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mYou cannot execute `.\u001b[39m\u001b[39m{\u001b[39;00mfn\u001b[39m}\u001b[39;00m\u001b[39m(ckpt_path=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbest\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m)` with `fast_dev_run=True`.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    148\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Please pass an exact checkpoint path to `.\u001b[39m\u001b[39m{\u001b[39;00mfn\u001b[39m}\u001b[39;00m\u001b[39m(ckpt_path=...)`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         )\n\u001b[1;32m--> 150\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    151\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m`.\u001b[39m\u001b[39m{\u001b[39;00mfn\u001b[39m}\u001b[39;00m\u001b[39m(ckpt_path=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbest\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m)` is set but `ModelCheckpoint` is not configured to save the best model.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    152\u001b[0m     )\n\u001b[0;32m    153\u001b[0m \u001b[39m# load best weights\u001b[39;00m\n\u001b[0;32m    154\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mcheckpoint_callback, \u001b[39m\"\u001b[39m\u001b[39mbest_model_path\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: `.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model."
     ]
    }
   ],
   "source": [
    "trainer.test(model=hcdrclassifier, datamodule=HCDR_DataModule(), ckpt_path='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import AUROC\n",
    "\n",
    "class MultiLayerPerceptron_2(pl.LightningModule):\n",
    "    def __init__(self, input_size=(1429), hidden_units=(40, 24, 16)):\n",
    "        super().__init__()\n",
    "        \n",
    "        # new PL attributes:\n",
    "        self.train_AUC = AUROC(\"binary\", num_classes=2)  #change to binary class and 2 for HCDR\n",
    "        self.valid_AUC = AUROC(\"binary\", num_classes=2)\n",
    "        self.test_AUC = AUROC(\"binary\",num_classes=2)\n",
    "        \n",
    "        # Model similar to previous section:\n",
    "        input_size = input_size\n",
    "        all_layers = [nn.Linear(input_size, hidden_units[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_units[0], hidden_units[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_units[1], hidden_units[2]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_units[2], 2),\n",
    "                      nn.Sigmoid()\n",
    "                      ]\n",
    "        # for hidden_unit in hidden_units: \n",
    " \n",
    "        # all_layers.append(nn.Linear(hidden_units[-1], 10)) \n",
    "        self.model = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # add in a loss layer\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y.squeeze().long())\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_AUC.update(preds, y.squeeze().long())\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outs):\n",
    "        self.log(\"train_AUC\", self.train_AUC.compute())\n",
    "        self.train_AUC.reset()\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y.squeeze().long())\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.valid_AUC.update(preds, y.squeeze().long())\n",
    "        self.log(\"valid_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_epoch_end(self, outs):\n",
    "        self.log(\"valid_AUC\", self.valid_AUC.compute(), prog_bar=True)\n",
    "        self.valid_AUC.reset()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y.squeeze().long())\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.test_AUC.update(preds, y.squeeze().long())\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_AUC\", self.test_AUC.compute(), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type        | Params\n",
      "------------------------------------------\n",
      "0 | train_AUC | BinaryAUROC | 0     \n",
      "1 | valid_AUC | BinaryAUROC | 0     \n",
      "2 | test_AUC  | BinaryAUROC | 0     \n",
      "3 | model     | Sequential  | 58.6 K\n",
      "------------------------------------------\n",
      "58.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "58.6 K    Total params\n",
      "0.234     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba40335a8824ff3b690a263d978cff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pvste\\anaconda3\\envs\\my_ml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4c44fae2994c60a66d5e6088144164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d56168c55b54c449c8e912b643d5d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5a3136d7ed426d96d8122893e630b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb400389a624885a73a820a1615bc29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c3f023de6e4a41a15b8a1623ce2d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1ffcc83a7243159fb875168229a311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7bb23d91174950bb1580eeb9312615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6e95ce2f8a4bac925784c0770f87bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d841320a10c484eb556619b219d2360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cf45f5dfb24e06a2e4041e3a68ac2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ba9ee117794b6b80c99a560032f786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60718a6b260b4615a21e7e49491478e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0524f367735942aeb33a48cac061bebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cec8fc217e14288bc62c31a976486e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca8e6fd83f647b599d42f67b5056989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025d507686374b87ac4d48f49c550e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hcdrclassifier_2 = MultiLayerPerceptron_2()\n",
    "\n",
    "callbacks = [ModelCheckpoint(save_top_k=1, mode='max', monitor=\"valid_AUC\")] # save top 1 model\n",
    "\n",
    "if torch.cuda.is_available(): # if you have GPUs\n",
    "    print(\"GPU is available\")\n",
    "    trainer = pl.Trainer(max_epochs=15, callbacks=callbacks, gpus=1)\n",
    "else:\n",
    "    trainer = pl.Trainer(max_epochs=15, callbacks=callbacks)\n",
    "\n",
    "trainer.fit(model=hcdrclassifier_2, datamodule=HCDR_DataModule())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at c:\\Users\\pvste\\Documents\\code\\lightning_logs\\version_6\\checkpoints\\epoch=0-step=13.ckpt\n",
      "Loaded model weights from checkpoint at c:\\Users\\pvste\\Documents\\code\\lightning_logs\\version_6\\checkpoints\\epoch=0-step=13.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f503848b0a4822b661f043044d1d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_AUC          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.5            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5737823843955994     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_AUC         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.5           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5737823843955994    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.5737823843955994, 'test_AUC': 0.5}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=hcdrclassifier_2, datamodule=HCDR_DataModule(), ckpt_path='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82818b14de67220291b5b43de7f72d0e85f2ba15f2bafdeb51a5520bc5ee5ebe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
